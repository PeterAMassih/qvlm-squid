# QVLM-SQuID

[![Hugging Face Dataset](https://img.shields.io/badge/Hugging%20Face-Dataset-yellow)](https://huggingface.co/datasets/PeterAM4/SQuID)
[![arXiv](https://img.shields.io/badge/arXiv-2601.13401-b31b1b.svg)](https://arxiv.org/abs/2601.13401)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)

Repo for the paper  
**â€œReasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analyticsâ€**  
including code for dataset generation, training, and evaluation.

---

## ğŸ”— Links

- ğŸ“¦ **SQuID Dataset (Hugging Face)**  
  https://huggingface.co/datasets/PeterAM4/SQuID

- ğŸ“„ **Paper (arXiv)**  
  https://arxiv.org/abs/2601.13401

---

## ğŸ“˜ Overview

This repository contains the implementation of **QVLM**, a code-generationâ€“based vision-language architecture for **quantitative spatial reasoning**, along with tools to generate and evaluate the **SQuID** benchmark.

QVLM preserves pixel-level precision by decoupling language understanding from visual analysis and operating directly on segmentation masks.

---

## ğŸ“š Citation

If you use this code or the dataset, please cite:

```bibtex
@misc{massih2026reasoningpixellevelprecisionqvlm,
  title={Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics},
  author={Peter A. Massih and Eric Cosatto},
  year={2026},
  eprint={2601.13401},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2601.13401}
}
